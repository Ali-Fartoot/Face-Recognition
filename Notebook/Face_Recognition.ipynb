{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36df81d",
   "metadata": {},
   "source": [
    "# import files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3965f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import uuid\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "from torch.utils.data import ConcatDataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efab7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join(\"..\",'data','positive')\n",
    "NEG_PATH = os.path.join(\"..\",'data','negetive')\n",
    "ANC_PATH = os.path.join(\"..\",'data','anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a698db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directories\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76dec2f",
   "metadata": {},
   "source": [
    "# Untar Labelled Faces in the Wild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1eed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw'):\n",
    "  for file in os.listdir(os.path.join('lfw',directory)):\n",
    "    EX_PATH = os.path.join('lfw',directory, file)\n",
    "    NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "    os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e415d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe1340",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0416502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "def data_aug(img):\n",
    "    flag = [0,0,0,0]\n",
    "\n",
    "    data = []\n",
    "    while not all(x == 1 for x in flag):\n",
    "        if flag[0] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_brightness(img, brightness_factor=1.1)\n",
    "            data.append(img)\n",
    "            flag[0] = 1\n",
    "        \n",
    "        if flag[1] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_contrast(img, contrast_factor=torch.empty(1).uniform_(0.6, 1).item())\n",
    "            data.append(img)\n",
    "            flag[1] = 1\n",
    "            \n",
    "        if flag[2] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.hflip(img)\n",
    "            data.append(img)\n",
    "            flag[2] = 1\n",
    "            \n",
    "        if flag[3] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_saturation(img, saturation_factor=torch.empty(1).uniform_(0.9, 1).item())\n",
    "            data.append(img)\n",
    "            flag[3] = 1\n",
    "        \n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e371fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = F.to_tensor(img)\n",
    "    augmented_images = data_aug(img_tensor)\n",
    "    for i, image in enumerate(augmented_images):\n",
    "        image = F.to_pil_image(image)\n",
    "        image.save(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e67961",
   "metadata": {},
   "source": [
    "# Load Data in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "02eb9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeImageDataset(Dataset):\n",
    "    def __init__(self,ANC_PATH  ,POS_PATH, NEG_PATH, types ,transform = None):\n",
    "        \n",
    "        self.POS_PATH = POS_PATH\n",
    "        self.NEG_PATH = NEG_PATH\n",
    "        self.ANC_PATH = ANC_PATH\n",
    "        \n",
    "        self.types = types\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.POS_IMG = os.listdir(POS_PATH)\n",
    "        self.NEG_IMG = os.listdir(NEG_PATH)\n",
    "        self.ANC_IMG = os.listdir(ANC_PATH)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ANC_IMG)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anc_dir  = os.path.join(self.ANC_PATH, self.ANC_IMG[idx])\n",
    "        anc_image = Image.open(anc_dir).convert('RGB')\n",
    "        \n",
    "        if self.types == 1: \n",
    "            pos_dir  = os.path.join(self.POS_PATH, self.POS_IMG[idx])\n",
    "            pos_image = Image.open(pos_dir).convert('RGB')\n",
    "\n",
    "        if self.types == 0: \n",
    "            neg_dir  = os.path.join(self.NEG_PATH, self.NEG_IMG[idx])\n",
    "            neg_image = Image.open(neg_dir).convert('RGB')\n",
    "        \n",
    "        data = [anc_image, pos_image if self.types == 1 else neg_image, torch.ones(1) if self.types == 1 else torch.zeros(1) ]\n",
    "\n",
    "        if self.transform:\n",
    "            data[0] = self.transform(data[0])\n",
    "            data[1] = self.transform(data[1])\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "27da1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ddb4a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = MergeImageDataset(ANC_PATH, POS_PATH, NEG_PATH, types = 1, transform = transform)\n",
    "negtive = MergeImageDataset(ANC_PATH, POS_PATH, NEG_PATH, types = 0, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9698b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ConcatDataset([negtive,positive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d38ee98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the individual datasets into training and validation parts\n",
    "num_samples = len(data)\n",
    "indices = list(range(num_samples))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * num_samples))  # Use 20% of data for validation\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Create samplers for the training and validation parts\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0d9fdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and validation parts\n",
    "train_loader = DataLoader(data, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(data, batch_size=32, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb250620",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a422261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
