{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36df81d",
   "metadata": {},
   "source": [
    "# import files and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3965f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import uuid\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "from torch.utils.data import ConcatDataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch import flatten\n",
    "import torch.nn.functional as Fnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efab7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join(\"..\",'data','positive')\n",
    "NEG_PATH = os.path.join(\"..\",'data','negetive')\n",
    "ANC_PATH = os.path.join(\"..\",'data','anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a698db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add directories\n",
    "os.makedirs(POS_PATH)\n",
    "os.makedirs(NEG_PATH)\n",
    "os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76dec2f",
   "metadata": {},
   "source": [
    "# Untar Labelled Faces in the Wild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1eed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in os.listdir('lfw'):\n",
    "  for file in os.listdir(os.path.join('lfw',directory)):\n",
    "    EX_PATH = os.path.join('lfw',directory, file)\n",
    "    NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "    os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e415d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[120:120+250,200:200+250, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out anchor image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1()))\n",
    "        # Write out positive image\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe1340",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0416502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "import random\n",
    "def data_aug(img):\n",
    "    flag = [0,0,0,0]\n",
    "\n",
    "    data = []\n",
    "    while not all(x == 1 for x in flag):\n",
    "        if flag[0] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_brightness(img, brightness_factor=1.1)\n",
    "            data.append(img)\n",
    "            flag[0] = 1\n",
    "        \n",
    "        if flag[1] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_contrast(img, contrast_factor=torch.empty(1).uniform_(0.6, 1).item())\n",
    "            data.append(img)\n",
    "            flag[1] = 1\n",
    "            \n",
    "        if flag[2] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.hflip(img)\n",
    "            data.append(img)\n",
    "            flag[2] = 1\n",
    "            \n",
    "        if flag[3] == 0 and random.randint(0,100) / 100 < 0.5:\n",
    "            img = F.adjust_saturation(img, saturation_factor=torch.empty(1).uniform_(0.9, 1).item())\n",
    "            data.append(img)\n",
    "            flag[3] = 1\n",
    "        \n",
    "        \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e371fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(POS_PATH)):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = F.to_tensor(img)\n",
    "    augmented_images = data_aug(img_tensor)\n",
    "    for i, image in enumerate(augmented_images):\n",
    "        image = F.to_pil_image(image)\n",
    "        image.save(os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e67961",
   "metadata": {},
   "source": [
    "# Load Data in Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02eb9539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeImageDataset(Dataset):\n",
    "    def __init__(self,ANC_PATH  ,POS_PATH, NEG_PATH, types ,transform = None):\n",
    "        \n",
    "        self.POS_PATH = POS_PATH\n",
    "        self.NEG_PATH = NEG_PATH\n",
    "        self.ANC_PATH = ANC_PATH\n",
    "        \n",
    "        self.types = types\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        self.POS_IMG = os.listdir(POS_PATH)\n",
    "        self.NEG_IMG = os.listdir(NEG_PATH)\n",
    "        self.ANC_IMG = os.listdir(ANC_PATH)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ANC_IMG)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anc_dir  = os.path.join(self.ANC_PATH, self.ANC_IMG[idx])\n",
    "        anc_image = Image.open(anc_dir).convert('RGB')\n",
    "        \n",
    "        if self.types == 1: \n",
    "            pos_dir  = os.path.join(self.POS_PATH, self.POS_IMG[idx])\n",
    "            pos_image = Image.open(pos_dir).convert('RGB')\n",
    "\n",
    "        if self.types == 0: \n",
    "            neg_dir  = os.path.join(self.NEG_PATH, self.NEG_IMG[idx])\n",
    "            neg_image = Image.open(neg_dir).convert('RGB')\n",
    "        \n",
    "        data = [anc_image, pos_image if self.types == 1 else neg_image, torch.ones(1) if self.types == 1 else torch.zeros(1) ]\n",
    "\n",
    "        if self.transform:\n",
    "            data[0] = self.transform(data[0])\n",
    "            data[1] = self.transform(data[1])\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27da1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb4a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = MergeImageDataset(ANC_PATH, POS_PATH, NEG_PATH, types = 1, transform = transform)\n",
    "negtive = MergeImageDataset(ANC_PATH, POS_PATH, NEG_PATH, types = 0, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9698b7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ConcatDataset([negtive,positive])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38ee98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the individual datasets into training and validation parts\n",
    "num_samples = len(data)\n",
    "indices = list(range(num_samples))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * num_samples))  # Use 20% of data for validation\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Create samplers for the training and validation parts\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9fdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for the training and validation parts\n",
    "train_loader = DataLoader(data, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(data, batch_size=32, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb250620",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50547c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a422261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "            super(Embedding, self).__init__()\n",
    "\n",
    "            self.conv_1 = nn.Conv2d(3, 64,kernel_size=(10, 10))\n",
    "            self.relu_1 = nn.ReLU()\n",
    "            self.maxpool_1 = nn.MaxPool2d(2, stride = 2 ,padding = 1)\n",
    "            \n",
    "            self.conv_2 =  nn.Conv2d(64, 128,kernel_size=(7, 7))\n",
    "            self.relu_2 =  nn.ReLU()\n",
    "            self.maxpool_2 = nn.MaxPool2d(2 , 2)\n",
    "            \n",
    "            self.conv_3 =  nn.Conv2d(128, 128,kernel_size=(4, 4))\n",
    "            self.relu_3 =  nn.ReLU()\n",
    "            self.maxpool_3 =  nn.MaxPool2d(2, 2, padding = 1)\n",
    "            \n",
    "            self.conv_4 =  nn.Conv2d(128, 256,kernel_size=(4, 4))\n",
    "            self.relu_4 =  nn.ReLU()\n",
    "            \n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Linear(256 * 6 * 6, 4096),\n",
    "                nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.maxpool_1(x)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu_2(x)\n",
    "        x = self.maxpool_2(x)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.relu_3(x)\n",
    "        x = self.maxpool_3(x)\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.relu_4(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)\n",
    "        print(x.shape)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c911be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_1: torch.Size([64, 91, 91])\n",
      "relu_1: torch.Size([64, 91, 91])\n",
      "maxpool_1: torch.Size([64, 46, 46])\n",
      "conv_2: torch.Size([128, 40, 40])\n",
      "relu_2: torch.Size([128, 40, 40])\n",
      "maxpool_2: torch.Size([128, 20, 20])\n",
      "conv_3: torch.Size([128, 17, 17])\n",
      "relu_3: torch.Size([128, 17, 17])\n",
      "maxpool_3: torch.Size([128, 9, 9])\n",
      "conv_4: torch.Size([256, 6, 6])\n",
      "relu_4: torch.Size([256, 6, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1536x6 and 9216x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23076\\833384998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{name}: {x.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1536x6 and 9216x4096)"
     ]
    }
   ],
   "source": [
    "model = Embedding()\n",
    "\n",
    "# Specify the input size\n",
    "input_size = (3, 100, 100)\n",
    "\n",
    "# Move the model to the desired device (e.g., CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "x = torch.randn(input_size).to(device)\n",
    "\n",
    "for name, layer in model.named_children():\n",
    "    x = layer(x)\n",
    "    print(f\"{name}: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106a42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f6715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
